{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:\n",
      "CUDA_VISIBLE_DEVICES=4,5,6,7 python ../benchmarks/benchmark_latency.py                --batch-size 1                --model \"meta-llama/Llama-3.1-70B-Instruct\"                --dataset-name sharegpt                --dataset-path \"/data/lily/ShareGPT_V3_unfiltered_cleaned_split.json\"                --output-json results/sharegpt_batch-size-1_ntokens-0.json --tp 4 \n",
      "\n",
      "INFO 04-08 10:50:13 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: benchmark_latency.py [-h] [--input-len INPUT_LEN]\n",
      "                            [--output-len OUTPUT_LEN]\n",
      "                            [--batch-size BATCH_SIZE] [--n N]\n",
      "                            [--use-beam-search]\n",
      "                            [--num-iters-warmup NUM_ITERS_WARMUP]\n",
      "                            [--num-iters NUM_ITERS] [--profile]\n",
      "                            [--profile-result-dir PROFILE_RESULT_DIR]\n",
      "                            [--output-json OUTPUT_JSON] [--disable-detokenize]\n",
      "                            [--dataset-name {sharegpt,random,sonnet,burstgpt,hf}]\n",
      "                            [--iterate-requests]\n",
      "                            [--random-range-ratio RANDOM_RANGE_RATIO]\n",
      "                            [--dataset-path DATASET_PATH]\n",
      "                            [--lora-path LORA_PATH] [--prefix-len PREFIX_LEN]\n",
      "                            [--model MODEL]\n",
      "                            [--task {auto,generate,embedding,embed,classify,score,reward,transcription}]\n",
      "                            [--tokenizer TOKENIZER]\n",
      "                            [--hf-config-path HF_CONFIG_PATH]\n",
      "                            [--skip-tokenizer-init] [--revision REVISION]\n",
      "                            [--code-revision CODE_REVISION]\n",
      "                            [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                            [--tokenizer-mode {auto,slow,mistral,custom}]\n",
      "                            [--trust-remote-code]\n",
      "                            [--allowed-local-media-path ALLOWED_LOCAL_MEDIA_PATH]\n",
      "                            [--download-dir DOWNLOAD_DIR]\n",
      "                            [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral,runai_streamer,fastsafetensors}]\n",
      "                            [--config-format {auto,hf,mistral}]\n",
      "                            [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                            [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]\n",
      "                            [--max-model-len MAX_MODEL_LEN]\n",
      "                            [--guided-decoding-backend GUIDED_DECODING_BACKEND]\n",
      "                            [--logits-processor-pattern LOGITS_PROCESSOR_PATTERN]\n",
      "                            [--model-impl {auto,vllm,transformers}]\n",
      "                            [--distributed-executor-backend {ray,mp,uni,external_launcher}]\n",
      "                            [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                            [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                            [--data-parallel-size DATA_PARALLEL_SIZE]\n",
      "                            [--enable-expert-parallel]\n",
      "                            [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                            [--ray-workers-use-nsight]\n",
      "                            [--block-size {8,16,32,64,128}]\n",
      "                            [--enable-prefix-caching | --no-enable-prefix-caching]\n",
      "                            [--prefix-caching-hash-algo {builtin,sha256}]\n",
      "                            [--disable-sliding-window]\n",
      "                            [--use-v2-block-manager]\n",
      "                            [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]\n",
      "                            [--seed SEED] [--swap-space SWAP_SPACE]\n",
      "                            [--cpu-offload-gb CPU_OFFLOAD_GB]\n",
      "                            [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                            [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]\n",
      "                            [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                            [--max-num-partial-prefills MAX_NUM_PARTIAL_PREFILLS]\n",
      "                            [--max-long-partial-prefills MAX_LONG_PARTIAL_PREFILLS]\n",
      "                            [--long-prefill-token-threshold LONG_PREFILL_TOKEN_THRESHOLD]\n",
      "                            [--max-num-seqs MAX_NUM_SEQS]\n",
      "                            [--max-logprobs MAX_LOGPROBS]\n",
      "                            [--disable-log-stats]\n",
      "                            [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,nvfp4,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n",
      "                            [--rope-scaling ROPE_SCALING]\n",
      "                            [--rope-theta ROPE_THETA]\n",
      "                            [--hf-overrides HF_OVERRIDES] [--enforce-eager]\n",
      "                            [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]\n",
      "                            [--disable-custom-all-reduce]\n",
      "                            [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                            [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                            [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                            [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n",
      "                            [--mm-processor-kwargs MM_PROCESSOR_KWARGS]\n",
      "                            [--disable-mm-preprocessor-cache] [--enable-lora]\n",
      "                            [--enable-lora-bias] [--max-loras MAX_LORAS]\n",
      "                            [--max-lora-rank MAX_LORA_RANK]\n",
      "                            [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                            [--lora-dtype {auto,float16,bfloat16}]\n",
      "                            [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]\n",
      "                            [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                            [--fully-sharded-loras] [--enable-prompt-adapter]\n",
      "                            [--max-prompt-adapters MAX_PROMPT_ADAPTERS]\n",
      "                            [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n",
      "                            [--device {auto,cuda,neuron,cpu,tpu,xpu,hpu}]\n",
      "                            [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n",
      "                            [--use-tqdm-on-load | --no-use-tqdm-on-load]\n",
      "                            [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]\n",
      "                            [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                            [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n",
      "                            [--speculative-config SPECULATIVE_CONFIG]\n",
      "                            [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n",
      "                            [--ignore-patterns IGNORE_PATTERNS]\n",
      "                            [--preemption-mode PREEMPTION_MODE]\n",
      "                            [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n",
      "                            [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n",
      "                            [--show-hidden-metrics-for-version SHOW_HIDDEN_METRICS_FOR_VERSION]\n",
      "                            [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n",
      "                            [--collect-detailed-traces COLLECT_DETAILED_TRACES]\n",
      "                            [--disable-async-output-proc]\n",
      "                            [--scheduling-policy {fcfs,priority}]\n",
      "                            [--scheduler-cls SCHEDULER_CLS]\n",
      "                            [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n",
      "                            [--override-pooler-config OVERRIDE_POOLER_CONFIG]\n",
      "                            [--compilation-config COMPILATION_CONFIG]\n",
      "                            [--kv-transfer-config KV_TRANSFER_CONFIG]\n",
      "                            [--worker-cls WORKER_CLS]\n",
      "                            [--worker-extension-cls WORKER_EXTENSION_CLS]\n",
      "                            [--generation-config GENERATION_CONFIG]\n",
      "                            [--override-generation-config OVERRIDE_GENERATION_CONFIG]\n",
      "                            [--enable-sleep-mode] [--calculate-kv-scales]\n",
      "                            [--additional-config ADDITIONAL_CONFIG]\n",
      "                            [--enable-reasoning]\n",
      "                            [--reasoning-parser {deepseek_r1,granite}]\n",
      "                            [--disable-cascade-attn]\n",
      "benchmark_latency.py: error: unrecognized arguments: --tp 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:\n",
      "CUDA_VISIBLE_DEVICES=4,5,6,7 python ../benchmarks/benchmark_latency.py                --batch-size 1                --model \"meta-llama/Llama-3.1-70B-Instruct\"                --dataset-name sharegpt                --dataset-path \"/data/lily/ShareGPT_V3_unfiltered_cleaned_split.json\"                --output-json results/sharegpt_batch-size-1_ntokens-3.json --tp 4  --speculative-config '{\n",
      "                \"model\": \"ngram\",\n",
      "                \"prompt_lookup_max\": 7,\n",
      "                \"prompt_lookup_min\": 3,\n",
      "                \"num_speculative_tokens\": 3\n",
      "            }'\n",
      "\n",
      "INFO 04-08 10:50:20 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: benchmark_latency.py [-h] [--input-len INPUT_LEN]\n",
      "                            [--output-len OUTPUT_LEN]\n",
      "                            [--batch-size BATCH_SIZE] [--n N]\n",
      "                            [--use-beam-search]\n",
      "                            [--num-iters-warmup NUM_ITERS_WARMUP]\n",
      "                            [--num-iters NUM_ITERS] [--profile]\n",
      "                            [--profile-result-dir PROFILE_RESULT_DIR]\n",
      "                            [--output-json OUTPUT_JSON] [--disable-detokenize]\n",
      "                            [--dataset-name {sharegpt,random,sonnet,burstgpt,hf}]\n",
      "                            [--iterate-requests]\n",
      "                            [--random-range-ratio RANDOM_RANGE_RATIO]\n",
      "                            [--dataset-path DATASET_PATH]\n",
      "                            [--lora-path LORA_PATH] [--prefix-len PREFIX_LEN]\n",
      "                            [--model MODEL]\n",
      "                            [--task {auto,generate,embedding,embed,classify,score,reward,transcription}]\n",
      "                            [--tokenizer TOKENIZER]\n",
      "                            [--hf-config-path HF_CONFIG_PATH]\n",
      "                            [--skip-tokenizer-init] [--revision REVISION]\n",
      "                            [--code-revision CODE_REVISION]\n",
      "                            [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                            [--tokenizer-mode {auto,slow,mistral,custom}]\n",
      "                            [--trust-remote-code]\n",
      "                            [--allowed-local-media-path ALLOWED_LOCAL_MEDIA_PATH]\n",
      "                            [--download-dir DOWNLOAD_DIR]\n",
      "                            [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral,runai_streamer,fastsafetensors}]\n",
      "                            [--config-format {auto,hf,mistral}]\n",
      "                            [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                            [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]\n",
      "                            [--max-model-len MAX_MODEL_LEN]\n",
      "                            [--guided-decoding-backend GUIDED_DECODING_BACKEND]\n",
      "                            [--logits-processor-pattern LOGITS_PROCESSOR_PATTERN]\n",
      "                            [--model-impl {auto,vllm,transformers}]\n",
      "                            [--distributed-executor-backend {ray,mp,uni,external_launcher}]\n",
      "                            [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                            [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                            [--data-parallel-size DATA_PARALLEL_SIZE]\n",
      "                            [--enable-expert-parallel]\n",
      "                            [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                            [--ray-workers-use-nsight]\n",
      "                            [--block-size {8,16,32,64,128}]\n",
      "                            [--enable-prefix-caching | --no-enable-prefix-caching]\n",
      "                            [--prefix-caching-hash-algo {builtin,sha256}]\n",
      "                            [--disable-sliding-window]\n",
      "                            [--use-v2-block-manager]\n",
      "                            [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]\n",
      "                            [--seed SEED] [--swap-space SWAP_SPACE]\n",
      "                            [--cpu-offload-gb CPU_OFFLOAD_GB]\n",
      "                            [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                            [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]\n",
      "                            [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                            [--max-num-partial-prefills MAX_NUM_PARTIAL_PREFILLS]\n",
      "                            [--max-long-partial-prefills MAX_LONG_PARTIAL_PREFILLS]\n",
      "                            [--long-prefill-token-threshold LONG_PREFILL_TOKEN_THRESHOLD]\n",
      "                            [--max-num-seqs MAX_NUM_SEQS]\n",
      "                            [--max-logprobs MAX_LOGPROBS]\n",
      "                            [--disable-log-stats]\n",
      "                            [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,nvfp4,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n",
      "                            [--rope-scaling ROPE_SCALING]\n",
      "                            [--rope-theta ROPE_THETA]\n",
      "                            [--hf-overrides HF_OVERRIDES] [--enforce-eager]\n",
      "                            [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]\n",
      "                            [--disable-custom-all-reduce]\n",
      "                            [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                            [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                            [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                            [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n",
      "                            [--mm-processor-kwargs MM_PROCESSOR_KWARGS]\n",
      "                            [--disable-mm-preprocessor-cache] [--enable-lora]\n",
      "                            [--enable-lora-bias] [--max-loras MAX_LORAS]\n",
      "                            [--max-lora-rank MAX_LORA_RANK]\n",
      "                            [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                            [--lora-dtype {auto,float16,bfloat16}]\n",
      "                            [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]\n",
      "                            [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                            [--fully-sharded-loras] [--enable-prompt-adapter]\n",
      "                            [--max-prompt-adapters MAX_PROMPT_ADAPTERS]\n",
      "                            [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n",
      "                            [--device {auto,cuda,neuron,cpu,tpu,xpu,hpu}]\n",
      "                            [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n",
      "                            [--use-tqdm-on-load | --no-use-tqdm-on-load]\n",
      "                            [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]\n",
      "                            [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                            [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n",
      "                            [--speculative-config SPECULATIVE_CONFIG]\n",
      "                            [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n",
      "                            [--ignore-patterns IGNORE_PATTERNS]\n",
      "                            [--preemption-mode PREEMPTION_MODE]\n",
      "                            [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n",
      "                            [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n",
      "                            [--show-hidden-metrics-for-version SHOW_HIDDEN_METRICS_FOR_VERSION]\n",
      "                            [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n",
      "                            [--collect-detailed-traces COLLECT_DETAILED_TRACES]\n",
      "                            [--disable-async-output-proc]\n",
      "                            [--scheduling-policy {fcfs,priority}]\n",
      "                            [--scheduler-cls SCHEDULER_CLS]\n",
      "                            [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n",
      "                            [--override-pooler-config OVERRIDE_POOLER_CONFIG]\n",
      "                            [--compilation-config COMPILATION_CONFIG]\n",
      "                            [--kv-transfer-config KV_TRANSFER_CONFIG]\n",
      "                            [--worker-cls WORKER_CLS]\n",
      "                            [--worker-extension-cls WORKER_EXTENSION_CLS]\n",
      "                            [--generation-config GENERATION_CONFIG]\n",
      "                            [--override-generation-config OVERRIDE_GENERATION_CONFIG]\n",
      "                            [--enable-sleep-mode] [--calculate-kv-scales]\n",
      "                            [--additional-config ADDITIONAL_CONFIG]\n",
      "                            [--enable-reasoning]\n",
      "                            [--reasoning-parser {deepseek_r1,granite}]\n",
      "                            [--disable-cascade-attn]\n",
      "benchmark_latency.py: error: unrecognized arguments: --tp 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:\n",
      "CUDA_VISIBLE_DEVICES=4,5,6,7 python ../benchmarks/benchmark_latency.py                --batch-size 1                --model \"meta-llama/Llama-3.1-70B-Instruct\"                --dataset-name sharegpt                --dataset-path \"/data/lily/ShareGPT_V3_unfiltered_cleaned_split.json\"                --output-json results/sharegpt_batch-size-1_ntokens-5.json --tp 4  --speculative-config '{\n",
      "                \"model\": \"ngram\",\n",
      "                \"prompt_lookup_max\": 7,\n",
      "                \"prompt_lookup_min\": 3,\n",
      "                \"num_speculative_tokens\": 5\n",
      "            }'\n",
      "\n",
      "INFO 04-08 10:50:26 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: benchmark_latency.py [-h] [--input-len INPUT_LEN]\n",
      "                            [--output-len OUTPUT_LEN]\n",
      "                            [--batch-size BATCH_SIZE] [--n N]\n",
      "                            [--use-beam-search]\n",
      "                            [--num-iters-warmup NUM_ITERS_WARMUP]\n",
      "                            [--num-iters NUM_ITERS] [--profile]\n",
      "                            [--profile-result-dir PROFILE_RESULT_DIR]\n",
      "                            [--output-json OUTPUT_JSON] [--disable-detokenize]\n",
      "                            [--dataset-name {sharegpt,random,sonnet,burstgpt,hf}]\n",
      "                            [--iterate-requests]\n",
      "                            [--random-range-ratio RANDOM_RANGE_RATIO]\n",
      "                            [--dataset-path DATASET_PATH]\n",
      "                            [--lora-path LORA_PATH] [--prefix-len PREFIX_LEN]\n",
      "                            [--model MODEL]\n",
      "                            [--task {auto,generate,embedding,embed,classify,score,reward,transcription}]\n",
      "                            [--tokenizer TOKENIZER]\n",
      "                            [--hf-config-path HF_CONFIG_PATH]\n",
      "                            [--skip-tokenizer-init] [--revision REVISION]\n",
      "                            [--code-revision CODE_REVISION]\n",
      "                            [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                            [--tokenizer-mode {auto,slow,mistral,custom}]\n",
      "                            [--trust-remote-code]\n",
      "                            [--allowed-local-media-path ALLOWED_LOCAL_MEDIA_PATH]\n",
      "                            [--download-dir DOWNLOAD_DIR]\n",
      "                            [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral,runai_streamer,fastsafetensors}]\n",
      "                            [--config-format {auto,hf,mistral}]\n",
      "                            [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                            [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]\n",
      "                            [--max-model-len MAX_MODEL_LEN]\n",
      "                            [--guided-decoding-backend GUIDED_DECODING_BACKEND]\n",
      "                            [--logits-processor-pattern LOGITS_PROCESSOR_PATTERN]\n",
      "                            [--model-impl {auto,vllm,transformers}]\n",
      "                            [--distributed-executor-backend {ray,mp,uni,external_launcher}]\n",
      "                            [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                            [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                            [--data-parallel-size DATA_PARALLEL_SIZE]\n",
      "                            [--enable-expert-parallel]\n",
      "                            [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                            [--ray-workers-use-nsight]\n",
      "                            [--block-size {8,16,32,64,128}]\n",
      "                            [--enable-prefix-caching | --no-enable-prefix-caching]\n",
      "                            [--prefix-caching-hash-algo {builtin,sha256}]\n",
      "                            [--disable-sliding-window]\n",
      "                            [--use-v2-block-manager]\n",
      "                            [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]\n",
      "                            [--seed SEED] [--swap-space SWAP_SPACE]\n",
      "                            [--cpu-offload-gb CPU_OFFLOAD_GB]\n",
      "                            [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                            [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]\n",
      "                            [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                            [--max-num-partial-prefills MAX_NUM_PARTIAL_PREFILLS]\n",
      "                            [--max-long-partial-prefills MAX_LONG_PARTIAL_PREFILLS]\n",
      "                            [--long-prefill-token-threshold LONG_PREFILL_TOKEN_THRESHOLD]\n",
      "                            [--max-num-seqs MAX_NUM_SEQS]\n",
      "                            [--max-logprobs MAX_LOGPROBS]\n",
      "                            [--disable-log-stats]\n",
      "                            [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,nvfp4,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n",
      "                            [--rope-scaling ROPE_SCALING]\n",
      "                            [--rope-theta ROPE_THETA]\n",
      "                            [--hf-overrides HF_OVERRIDES] [--enforce-eager]\n",
      "                            [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]\n",
      "                            [--disable-custom-all-reduce]\n",
      "                            [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                            [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                            [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                            [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n",
      "                            [--mm-processor-kwargs MM_PROCESSOR_KWARGS]\n",
      "                            [--disable-mm-preprocessor-cache] [--enable-lora]\n",
      "                            [--enable-lora-bias] [--max-loras MAX_LORAS]\n",
      "                            [--max-lora-rank MAX_LORA_RANK]\n",
      "                            [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                            [--lora-dtype {auto,float16,bfloat16}]\n",
      "                            [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]\n",
      "                            [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                            [--fully-sharded-loras] [--enable-prompt-adapter]\n",
      "                            [--max-prompt-adapters MAX_PROMPT_ADAPTERS]\n",
      "                            [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n",
      "                            [--device {auto,cuda,neuron,cpu,tpu,xpu,hpu}]\n",
      "                            [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n",
      "                            [--use-tqdm-on-load | --no-use-tqdm-on-load]\n",
      "                            [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]\n",
      "                            [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                            [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n",
      "                            [--speculative-config SPECULATIVE_CONFIG]\n",
      "                            [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n",
      "                            [--ignore-patterns IGNORE_PATTERNS]\n",
      "                            [--preemption-mode PREEMPTION_MODE]\n",
      "                            [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n",
      "                            [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n",
      "                            [--show-hidden-metrics-for-version SHOW_HIDDEN_METRICS_FOR_VERSION]\n",
      "                            [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n",
      "                            [--collect-detailed-traces COLLECT_DETAILED_TRACES]\n",
      "                            [--disable-async-output-proc]\n",
      "                            [--scheduling-policy {fcfs,priority}]\n",
      "                            [--scheduler-cls SCHEDULER_CLS]\n",
      "                            [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n",
      "                            [--override-pooler-config OVERRIDE_POOLER_CONFIG]\n",
      "                            [--compilation-config COMPILATION_CONFIG]\n",
      "                            [--kv-transfer-config KV_TRANSFER_CONFIG]\n",
      "                            [--worker-cls WORKER_CLS]\n",
      "                            [--worker-extension-cls WORKER_EXTENSION_CLS]\n",
      "                            [--generation-config GENERATION_CONFIG]\n",
      "                            [--override-generation-config OVERRIDE_GENERATION_CONFIG]\n",
      "                            [--enable-sleep-mode] [--calculate-kv-scales]\n",
      "                            [--additional-config ADDITIONAL_CONFIG]\n",
      "                            [--enable-reasoning]\n",
      "                            [--reasoning-parser {deepseek_r1,granite}]\n",
      "                            [--disable-cascade-attn]\n",
      "benchmark_latency.py: error: unrecognized arguments: --tp 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"ENABLE_PROFILING\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = \"/data/lily\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "batch_sizes = [1]\n",
    "num_speculative_tokens_list = [3, 5]\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "dataset_name = \"sharegpt\"\n",
    "dataset_path = \"/data/lily/ShareGPT_V3_unfiltered_cleaned_split.json\"\n",
    "\n",
    "# dataset_name = \"sonnet\"\n",
    "# dataset_path = \"/data/lily/vllm-eagle-weight/benchmarks/sonnet.txt\"\n",
    "\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for num_tokens in num_speculative_tokens_list:\n",
    "        output_json = f\"results/{dataset_name}_batch-size-{batch_size}_ntokens-{num_tokens}.json\"\n",
    "        \n",
    "        cmd = f'''CUDA_VISIBLE_DEVICES=4,5,6,7 python ../benchmarks/benchmark_latency.py \\\n",
    "               --batch-size {batch_size} \\\n",
    "               --model \"{model_name}\" \\\n",
    "               --dataset-name {dataset_name} \\\n",
    "               --dataset-path \"{dataset_path}\" \\\n",
    "               --output-json {output_json}''' \n",
    "        \n",
    "        if '70' in model_name:\n",
    "            cmd += \" --tensor-parallel-size 4 \"\n",
    "            \n",
    "        if num_tokens > 0:\n",
    "            speculative_config = f'''{{\n",
    "                \"model\": \"ngram\",\n",
    "                \"prompt_lookup_max\": 7,\n",
    "                \"prompt_lookup_min\": 3,\n",
    "                \"num_speculative_tokens\": {num_tokens}\n",
    "            }}'''\n",
    "            cmd += f\" --speculative-config '{speculative_config}'\"\n",
    "        \n",
    "        print(f\"Command:\\n{cmd}\\n\")\n",
    "        os.environ[\"VLLM_USE_V1\"] = \"1\" \n",
    "        os.system(cmd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle-weight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
